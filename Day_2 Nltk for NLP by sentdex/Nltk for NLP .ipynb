{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nltk for NLP by www.youtube.com/sentdex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 1 - Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "example_text = \"December, it unexpectedly headed southeast toward Futuna. The system peaked at Category 3 on 28 December, with sustained winds of around 150 km/h (90 mph). It turned southwest the next day, toward Fiji and several smaller islands in the Lau group. The storm dissipated on 5 January over the north Tasman Sea. Raja caused two deaths as it impacted the island nations of Tuvalu, Wallis and Futuna, Tonga and Fiji. Gusty winds and rough seas caused extensive damage to crops, coastal installations and buildings in Tuvalu, and greater destruction in Futuna. Raja was responsible for the worst flood of the Labasa River in Fiji since 1929.\"\n",
    "\n",
    "# print (sent_tokenize(example_text))\n",
    "# print (word_tokenize(example_text))\n",
    "\n",
    "\n",
    "## word tokenizing \n",
    "for i in word_tokenize (example_text):\n",
    "    print (i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 2 - Stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence  = \"This is an example showing off stop word filtraions.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#print (stop_words)    #Full collection of english stopwords \n",
    "\n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "\n",
    "# filtered_sentence = []\n",
    "# for w in words :\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "# print (filtered_sentence)\n",
    "\n",
    "\n",
    "## also by using list comprehension\n",
    "filtered_sentence  =[w for w in words if not w in stop_words]\n",
    "print (filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Video 3 - Stemming  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing sentenence that have same meaning\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words  = [\"Pyhton\",\"Pythoner\",\"Pythoning\",\"Pythonly\"]\n",
    "\n",
    "# for w in example_words:\n",
    "#     print (ps.stem(w))\n",
    "\n",
    "\n",
    "new_text = \"It is very import to be pythonly while you are pythoning the python . All pyhton have pyhton at least poorly\"\n",
    "words    =  word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print (ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Video 4 - Parts of speech tagging  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 5  Chunking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\" Chunk: {<RB.?>*<VB,?>*<NNP.?>+<NN>?} \"\"\" #finding adverb,verb,propernoun,noun \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked    = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            #chunked.draw()  #for drawing graph of chunked data\n",
    "                       \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video-6 Chinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of something except something \n",
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\" Chunk: {<.*>+}\n",
    "                                     }<VB.?|IN|DT|TO>+{\"\"\"   #chunk everything and keep verb out or prepostion,deteminant \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked    = chunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "            chunked.draw()  #for drawing graph of chunked data\n",
    "                       \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 7 Name entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of something except something \n",
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt  = nltk.ne_chunk(tagged,binary = True ) #binary = true ,it classify everthing as name entity period\n",
    "            \n",
    "            #namedEnt.draw()\n",
    "            print(namedEnt)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Name Entity  Type and Examples\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video-8 lemmitizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# print (lemmatizer.lemmatize(\"cats\"))\n",
    "# print (lemmatizer.lemmatize(\"cacti\"))\n",
    "# print (lemmatizer.lemmatize(\"geeze\"))\n",
    "# print (lemmatizer.lemmatize(\"rocks\"))\n",
    "# print (lemmatizer.lemmatize(\"pythoning \"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print (lemmatizer.lemmatize(\"best\",pos=\"a\"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"ran\",\"v\"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"better\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  video - 9 corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample =  gutenberg.raw(\"bible-kjv.txt\")\n",
    "tok    = sent_tokenize(sample)\n",
    "\n",
    "print (tok[5:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 10  wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word which have synonyms,antonyms and defnations and even context \n",
    "from nltk.corpus import wordnet \n",
    "syns =  wordnet.synsets(\"program\")\n",
    "\n",
    "##synset\n",
    "print (syns[0])\n",
    "\n",
    "##just a word \n",
    "print (syns[0].lemmas()[0].name())\n",
    "\n",
    "       \n",
    "##examples\n",
    "print(syns[0].examples())\n",
    "\n",
    "\n",
    "synonyms  = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "\n",
    "print (set(synonyms))\n",
    "print (set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Semantic similarity #################\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "print (w1.wup_similarity(w2))\n",
    "\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "print (w1.wup_similarity(w2))\n",
    "        \n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "print (w1.wup_similarity(w2))\n",
    "\n",
    "w1 = wordnet.synset(\"lion.n.01\")\n",
    "w2 = wordnet.synset(\"lioness.n.01\")\n",
    "print (w1.wup_similarity(w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 11  Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "print (documents[3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# print (all_words.most_common(15))\n",
    "#print (all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video-12 Words  as Features for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_features =  list (all_words.keys())[:3000]\n",
    "\n",
    "def  find_features(document):\n",
    "    words =set (document)\n",
    "    features  = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features \n",
    "\n",
    "print ((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets =  [(find_features(rev), category) for (rev,category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Video 13  Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_features =  list (all_words.keys())[:3000]\n",
    "\n",
    "def  find_features(document):\n",
    "    words =set (document)\n",
    "    features  = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features \n",
    "\n",
    "print ((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets =  [(find_features(rev), category) for (rev,category) in documents]\n",
    "\n",
    "\n",
    "\n",
    "training_set = featuresets [:1900]\n",
    "test_set = featuresets [:1900]\n",
    " \n",
    "# naive bayes ----> posterior  =  (prior occureneces x likelihood) / evidence\n",
    "\n",
    "\n",
    "classifier =  nltk.NaiveBayesClassifier.train(training_set)\n",
    "print (\"Naive bayes alg accuracy percent: \", (nltk.classify.accuracy(classifier,test_set))*100)\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video 14 Save Classifier with pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle \n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "\n",
    "word_features =  list (all_words.keys())[:3000]\n",
    "\n",
    "def  find_features(document):\n",
    "    words =set (document)\n",
    "    features  = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features \n",
    "\n",
    "print ((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets =  [(find_features(rev), category) for (rev,category) in documents]\n",
    "\n",
    "\n",
    "\n",
    "training_set = featuresets [:1900]\n",
    "test_set = featuresets [:1900]\n",
    " \n",
    "# naive bayes ----> posterior  =  (prior occureneces x likelihood) / evidence\n",
    "#classifier =  nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier=pickle.load (classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "print (\"Naive bayes alg accuracy percent: \", (nltk.classify.accuracy(classifier,test_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "####### saving theclassifier in  pickle of naive bayes algo ########\n",
    "# save_classifier=open(\"naivebayes.pickle\",\"wb\")\n",
    "# pickle.dump(classifier,save_classifier)\n",
    "# save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 15 - Scikit-learn incorporation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle \n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "\n",
    "word_features =  list (all_words.keys())[:3000]\n",
    "\n",
    "def  find_features(document):\n",
    "    words =set (document)\n",
    "    features  = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features \n",
    "\n",
    "#print ((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets =  [(find_features(rev), category) for (rev,category) in documents]\n",
    "\n",
    "training_set = featuresets [:1900]\n",
    "test_set = featuresets [1900:]\n",
    " \n",
    "#### naive bayes ----> posterior  =  (prior occureneces x likelihood) / evidence\n",
    "\n",
    "# classifier =  nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier=pickle.load (classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "print (\"Original Naive bayes alg accuracy percent: \", (nltk.classify.accuracy(classifier,test_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "#training here using sklearn\n",
    "MNB_classifier  = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print (\"MNB classifier accuracy percent: \", (nltk.classify.accuracy(MNB_classifier,test_set))*100)\n",
    "\n",
    "# GaussianNB_classifier  = SklearnClassifier(GaussianNB())\n",
    "# GaussianNB_classifier.train(training_set)\n",
    "# print (\"GaussianNB classifier accuracy percent: \", (nltk.classify.accuracy(GaussianNB_classifier,test_set))*100)\n",
    "\n",
    "BernoulliNB_classifier  = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print (\"BernoulliNB classifier accuracy percent: \", (nltk.classify.accuracy(BernoulliNB_classifier,test_set))*100)\n",
    "\n",
    "\n",
    "#  LogisticRegression,SGDClassifier\n",
    "#  SVC,LinearSVC,NuSVC\n",
    "\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print (\"LogisticRegression_classifier accuracy percent: \", (nltk.classify.accuracy(LogisticRegression_classifier,test_set))*100)\n",
    "\n",
    "SGDClassifier_classifier  = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print (\"SGDClassifier classifier accuracy percent: \", (nltk.classify.accuracy(SGDClassifier_classifier,test_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, test_set))*100)\n",
    "\n",
    "LinearSVC_classifier  = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print (\"LinearSVC classifier accuracy percent: \", (nltk.classify.accuracy(LinearSVC_classifier,test_set))*100)\n",
    "\n",
    "NuSVC_classifier  = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print (\"NuSVC classifier accuracy percent: \", (nltk.classify.accuracy(NuSVC_classifier,test_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 16  Combining algo with vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own alogrithms \n",
    "\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle \n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "####### defining class for adding voting classifier here ##################3\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "\n",
    "## used for traing and testing  features of document\n",
    "documents  = [(list (movie_reviews.words(fileid )),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "## all words are in common list \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words     = nltk.FreqDist(all_words)\n",
    "word_features =  list (all_words.keys())[:3000]\n",
    "\n",
    "def  find_features(document):\n",
    "    words =set (document)\n",
    "    features  = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features \n",
    "\n",
    "\n",
    "#print ((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "featuresets =  [(find_features(rev), category) for (rev,category) in documents]\n",
    "training_set = featuresets [:1900]\n",
    "test_set = featuresets [1900:]\n",
    " \n",
    "#### naive bayes ----> posterior  =  (prior occureneces x likelihood) / evidence\n",
    "# classifier =  nltk.NaiveBayesClassifier.train(training_set)\n",
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier=pickle.load (classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "print (\"Original Naive bayes alg accuracy percent: \", (nltk.classify.accuracy(classifier,test_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################## training here using sklearn ##############################3\n",
    "MNB_classifier  = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print (\"MNB classifier accuracy percent: \", (nltk.classify.accuracy(MNB_classifier,test_set))*100)\n",
    "\n",
    "# GaussianNB_classifier  = SklearnClassifier(GaussianNB())\n",
    "# GaussianNB_classifier.train(training_set)\n",
    "# print (\"GaussianNB classifier accuracy percent: \", (nltk.classify.accuracy(GaussianNB_classifier,test_set))*100)\n",
    "\n",
    "BernoulliNB_classifier  = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print (\"BernoulliNB classifier accuracy percent: \", (nltk.classify.accuracy(BernoulliNB_classifier,test_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print (\"LogisticRegression_classifier accuracy percent: \", (nltk.classify.accuracy(LogisticRegression_classifier,test_set))*100)\n",
    "\n",
    "SGDClassifier_classifier  = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print (\"SGDClassifier classifier accuracy percent: \", (nltk.classify.accuracy(SGDClassifier_classifier,test_set))*100)\n",
    "\n",
    "# SVC_classifier = SklearnClassifier(SVC())\n",
    "# SVC_classifier.train(training_set)\n",
    "# print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier  = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print (\"LinearSVC classifier accuracy percent: \", (nltk.classify.accuracy(LinearSVC_classifier,test_set))*100)\n",
    "\n",
    "NuSVC_classifier  = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print (\"NuSVC classifier accuracy percent: \", (nltk.classify.accuracy(NuSVC_classifier,test_set))*100)\n",
    "\n",
    "\n",
    "\n",
    "########### printing the voting classifier accuracy here ############################################ \n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  LogisticRegression_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier ) \n",
    "\n",
    "\n",
    "print (\"Voted_classifier accuracy percent: \", (nltk.classify.accuracy(voted_classifier,test_set))*100)\n",
    "\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[0][0]),\"Confidence %: \",voted_classifier.confidence(test_set[0][0])*100)\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[1][0]),\"Confidence %: \",voted_classifier.confidence(test_set[1][0])*100)\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[2][0]),\"Confidence %: \",voted_classifier.confidence(test_set[2][0])*100)\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[3][0]),\"Confidence %: \",voted_classifier.confidence(test_set[3][0])*100)\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[4][0]),\"Confidence %: \",voted_classifier.confidence(test_set[4][0])*100)\n",
    "print (\"Classification: \",voted_classifier.classify(test_set[5][0]),\"Confidence %: \",voted_classifier.confidence(test_set[5][0])*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Video - 17  Investigating bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# #generating the random number size=100 \n",
    "# random_number = np.random.randint(10, size=(1, 100))\n",
    "# #for i in random_number: \n",
    "#     #print(i, end=\"\")\n",
    "\n",
    "\n",
    "# for i in random_number: \n",
    "#     #print(i, end=\"\")\n",
    "#     for num in i: \n",
    "#         # checking condition \n",
    "#         if num % 2 == 0: \n",
    "#            print( \"0\", end = \"\")\n",
    "#         else:\n",
    "#             print(\"1\",end = \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the 4 digit pattern 0 & 1 only  = 1001\n",
      "Random  9109162638538293828692270740214472503745984043032273593807005882684341408949657695003517577378586228\n",
      "Output  1101100010110011000010010100010010101101100001010011111001001000000101000101011011001111111110100000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "#Taking the input for the pattern of length 4 of where, odd = 1 and even = 0  \n",
    "val = input(\"enter the 4 digit pattern 0 & 1 only  = \") \n",
    "\n",
    "\n",
    "\n",
    "#random string generating with size = 100\n",
    "def Random_Generator(size=100, chars=string.digits):            \n",
    "    return ''.join(random.choice(chars) for a in range(size))\n",
    "s = Random_Generator()\n",
    "\n",
    "\n",
    "input_number=list(s)\n",
    "i=0\n",
    "while i<len(input_number):\n",
    "    input_number[i]=str(int(input_number[i])%2)\n",
    "    i=i+1\n",
    "    # print(i)\n",
    "output=\"\"\n",
    "output=output.join(input_number)\n",
    "\n",
    "\n",
    "############# Output  for analysis #################################\n",
    "## To print the random string generating with size = 100\n",
    "print(\"Random  \" + s)  \n",
    "## output for the required usecase \n",
    "print(\"Output  \" + output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
