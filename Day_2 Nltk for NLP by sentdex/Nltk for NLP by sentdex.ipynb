{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nltk for NLP by www.youtube.com/sentdex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 1 - Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December\n",
      ",\n",
      "it\n",
      "unexpectedly\n",
      "headed\n",
      "southeast\n",
      "toward\n",
      "Futuna\n",
      ".\n",
      "The\n",
      "system\n",
      "peaked\n",
      "at\n",
      "Category\n",
      "3\n",
      "on\n",
      "28\n",
      "December\n",
      ",\n",
      "with\n",
      "sustained\n",
      "winds\n",
      "of\n",
      "around\n",
      "150\n",
      "km/h\n",
      "(\n",
      "90\n",
      "mph\n",
      ")\n",
      ".\n",
      "It\n",
      "turned\n",
      "southwest\n",
      "the\n",
      "next\n",
      "day\n",
      ",\n",
      "toward\n",
      "Fiji\n",
      "and\n",
      "several\n",
      "smaller\n",
      "islands\n",
      "in\n",
      "the\n",
      "Lau\n",
      "group\n",
      ".\n",
      "The\n",
      "storm\n",
      "dissipated\n",
      "on\n",
      "5\n",
      "January\n",
      "over\n",
      "the\n",
      "north\n",
      "Tasman\n",
      "Sea\n",
      ".\n",
      "Raja\n",
      "caused\n",
      "two\n",
      "deaths\n",
      "as\n",
      "it\n",
      "impacted\n",
      "the\n",
      "island\n",
      "nations\n",
      "of\n",
      "Tuvalu\n",
      ",\n",
      "Wallis\n",
      "and\n",
      "Futuna\n",
      ",\n",
      "Tonga\n",
      "and\n",
      "Fiji\n",
      ".\n",
      "Gusty\n",
      "winds\n",
      "and\n",
      "rough\n",
      "seas\n",
      "caused\n",
      "extensive\n",
      "damage\n",
      "to\n",
      "crops\n",
      ",\n",
      "coastal\n",
      "installations\n",
      "and\n",
      "buildings\n",
      "in\n",
      "Tuvalu\n",
      ",\n",
      "and\n",
      "greater\n",
      "destruction\n",
      "in\n",
      "Futuna\n",
      ".\n",
      "Raja\n",
      "was\n",
      "responsible\n",
      "for\n",
      "the\n",
      "worst\n",
      "flood\n",
      "of\n",
      "the\n",
      "Labasa\n",
      "River\n",
      "in\n",
      "Fiji\n",
      "since\n",
      "1929\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "example_text = \"December, it unexpectedly headed southeast toward Futuna. The system peaked at Category 3 on 28 December, with sustained winds of around 150 km/h (90 mph). It turned southwest the next day, toward Fiji and several smaller islands in the Lau group. The storm dissipated on 5 January over the north Tasman Sea. Raja caused two deaths as it impacted the island nations of Tuvalu, Wallis and Futuna, Tonga and Fiji. Gusty winds and rough seas caused extensive damage to crops, coastal installations and buildings in Tuvalu, and greater destruction in Futuna. Raja was responsible for the worst flood of the Labasa River in Fiji since 1929.\"\n",
    "\n",
    "# print (sent_tokenize(example_text))\n",
    "# print (word_tokenize(example_text))\n",
    "\n",
    "\n",
    "## word tokenizing \n",
    "for i in word_tokenize (example_text):\n",
    "    print (i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 2 - Stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtraions', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence  = \"This is an example showing off stop word filtraions.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#print (stop_words)    #Full collection of english stopwords \n",
    "\n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "\n",
    "# filtered_sentence = []\n",
    "# for w in words :\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "# print (filtered_sentence)\n",
    "\n",
    "\n",
    "## also by using list comprehension\n",
    "filtered_sentence  =[w for w in words if not w in stop_words]\n",
    "print (filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Video 3 - Stemming  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "the\n",
      "python\n",
      ".\n",
      "all\n",
      "pyhton\n",
      "have\n",
      "pyhton\n",
      "at\n",
      "least\n",
      "poorli\n"
     ]
    }
   ],
   "source": [
    "# removing sentenence that have same meaning\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words  = [\"Pyhton\",\"Pythoner\",\"Pythoning\",\"Pythonly\"]\n",
    "\n",
    "# for w in example_words:\n",
    "#     print (ps.stem(w))\n",
    "\n",
    "\n",
    "new_text = \"It is very import to be pythonly while you are pythoning the python . All pyhton have pyhton at least poorly\"\n",
    "words    =  word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print (ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Video 4 - Parts of speech tagging  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 5  Chunking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  called/VBD\n",
      "  (Chunk America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\" Chunk: {<RB.?>*<VB,?>*<NNP.?>+<NN>?} \"\"\" #finding adverb,verb,propernoun,noun \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked    = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            #chunked.draw()  #for drawing graph of chunked data\n",
    "                       \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video-6 Chinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of something except something \n",
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\" Chunk: {<.*>+}\n",
    "                                     }<VB.?|IN|DT|TO>+{\"\"\"   #chunk everything and keep verb out or prepostion,deteminant \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked    = chunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "            chunked.draw()  #for drawing graph of chunked data\n",
    "                       \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 7 Name entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nName Entity  Type and Examples\\nORGANIZATION - Georgia-Pacific Corp., WHO\\nPERSON - Eddy Bonte, President Obama\\nLOCATION - Murray River, Mount Everest\\nDATE - June, 2008-06-29\\nTIME - two fifty a m, 1:30 p.m.\\nMONEY - 175 million Canadian Dollars, GBP 10.40\\nPERCENT - twenty pct, 18.75 %\\nFACILITY - Washington Monument, Stonehenge\\nGPE - South East Asia, Midlothian\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removal of something except something \n",
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize  import PunktSentenceTokenizer\n",
    "\n",
    "train_text  =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text =  state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer =  PunktSentenceTokenizer(train_text)\n",
    "tokenized  =  custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt  = nltk.ne_chunk(tagged,binary = True ) #binary = true ,it classify everthing as name entity period\n",
    "            \n",
    "            #namedEnt.draw()\n",
    "            print(namedEnt)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Name Entity  Type and Examples\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video-8 lemmitizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "run\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# print (lemmatizer.lemmatize(\"cats\"))\n",
    "# print (lemmatizer.lemmatize(\"cacti\"))\n",
    "# print (lemmatizer.lemmatize(\"geeze\"))\n",
    "# print (lemmatizer.lemmatize(\"rocks\"))\n",
    "# print (lemmatizer.lemmatize(\"pythoning \"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print (lemmatizer.lemmatize(\"best\",pos=\"a\"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"ran\",\"v\"))\n",
    "\n",
    "print (lemmatizer.lemmatize(\"better\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  video - 9 corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample =  gutenberg.raw(\"bible-kjv.txt\")\n",
    "tok    = sent_tokenize(sample)\n",
    "\n",
    "print (tok[5:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video - 10  wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
